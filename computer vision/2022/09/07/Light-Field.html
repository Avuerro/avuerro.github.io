
<!-- Post Layout Start -->

<!DOCTYPE html>
<html lang="en">

  
<!-- HEAD Start -->

<head>
  


  <!-- Force HTTPS Start -->
  <script>
  // Don't force http when serving the website locally
  if (!(window.location.host.startsWith("127.0.0.1")) && (window.location.protocol != "https:"))
    window.location.protocol = "https";
  </script>

  <!-- Force HTTPS End -->



  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="My personal blog!">
  <meta name="author" content="Chihab Amghane">
  <meta name="keywords" content="AI, Tech, Computer Vision">
  <link rel="canonical" href="/computer%20vision/2022/09/07/Light-Field.html">
  <title>{ Chihab Amghane } | Computer Science approximation of the human visual system</title>

  <!-- Bootstrap Core CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">

  <!-- Custom CSS -->
  <link href="/css/grayscale.css" rel="stylesheet">
  

  <!-- Custom Fonts -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/rrssb.css" />
  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->
  
    <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico">
  

  

  


  <!-- iOS Web App mode -->

  <meta name="apple-mobile-web-app-capable" content="yes">
  <link rel="apple-touch-icon" sizes="36x36" href="/img/web-app/icon-36p.png">
  <link rel="apple-touch-icon" sizes="48x48" href="/img/web-app/icon-48p.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/img/web-app/iconTrue72p.png">
  <link rel="apple-touch-icon" sizes="96x96" href="/img/web-app/icon-96p.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/img/web-app/icon-144p.png">
  <link rel="apple-touch-icon" sizes="192x192" href="/img/web-app/icon-192p.png">

  <!-- Android Web App mode -->

  <link rel="manifest" href="/manifest.json">




  
<!-- Chrome, Firefox OS and Opera -->
<meta name="theme-color" content="#000000">
<!-- Windows Phone -->
<meta name="msapplication-navbutton-color" content="#000000">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">


  


  <!-- Syntax highlight in post pages -->

  <link rel="stylesheet" type="text/css" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.2/styles/monokai_sublime.min.css">




</head>

<!-- HEAD End -->


  <body>

    
<!-- Navigation Start -->

<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
        <i class="fa fa-bars"></i>
      </button>
      
        <a class="navbar-brand" href="/">
      
          <div>
            
              <img src="/img/black-lab-glass.ico" alt="">
            
	    
	      <img id="logo_nav" src="//img/author.png" alt="{ Chihab Amghane }" />
	    
              <!-- { Chihab Amghane } -->
          </div>
        </a>
    </div>
    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
      <ul class="nav navbar-nav">
        
          <!-- Blog, Post, Tag and Error pages -->
          
            <li>
            
                <a href="/#about"> About </a>
            
            </li>
          
            <li>
            
              
                <a href="/blog/"> Blog </a>
              
            
            </li>
          
            <li>
            
                <a href="/#contact"> Contact </a>
            
            </li>
          
        
      </ul>
    </div>
  </div>
</nav>

<!-- Navigation End -->


    <section id="post" class="container content-section text-center">
      <div class="row">
        <div class="col-md-10 col-md-offset-1">

          
<!-- Swipe Instructions Start -->

<div id="swipe-instruction">
  <div>
    <p><br><br><br></p>
    <i id="hand-swipe" class="fa fa-hand-o-up"></i>
    <p><strong>
    
      Did you know that you can navigate the posts by swiping left and right?
    
    </strong></p>
    <button type="button" class="btn btn-default ok-btn close-swipe-instruction">
      
        Awesome!
      
    </button>
  </div>
</div>

<!-- Swipe Instructions End -->


          <h1><strong>Computer Science approximation of the human visual system</strong></h1>
          <h4>
            <strong>07 Sep 2022</strong>
            <small>
              . category:
              <a class="category" href="/categories/computer vision.html">
                Computer Vision
              </a>.
              <a href="/computer%20vision/2022/09/07/Light-Field.html#disqus_thread">Comments</a>
              <br />
              
                <a class="tag" href="/tags/plenoptic function.html">#plenoptic function</a>
              
                <a class="tag" href="/tags/light field function.html">#light field function</a>
              
                <a class="tag" href="/tags/lumigraph.html">#lumigraph</a>
              
            </small>
          </h4>

          <section class="text-justify">
            <style>
 a{
  color:red;
 }
</style>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            displayMath: [['$$','$$']],
            inlineMath: [['$','$']],
        },
    });
</script>

<h1 id="light-field-function">Light Field Function</h1>

<p>The light field function has its origins in the plenoptic function. 
The two concepts are often used together or even mixed up which might be confusing. 
In order to understand the light field function we would first have to understand the concept on which it is based, namely the plenoptic function.</p>

<h2 id="plenoptic-function">Plenoptic Function</h2>
<p>The plenoptic function was introduced by Adelson and Bergen in 1991 [1]. 
The idea behind the plenoptic function is that it is a systematic overview of how the structures in the human visual system relate to the visual information in the world.</p>

<blockquote>
  <p>If you’re familiar with the <a href="https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis">levels of Marr</a>, then we could argue that the plenoptic function is at the computational level.</p>
</blockquote>

<p>Since we are dealing with the human visual system it is only natural to use the human eye as an observer. 
The rays of light in a scene pass through the pupil of the eye and the ability to capture visual information is therefore limited in the direction from which the light rays originate.
Or in other words, you cannot see through the back of your head.
However, let’s remove this limitation and add the assumption that we have an eye that is capable of capturing light from all directions (360 $^\circ$). 
We have now arrived at the plenoptic function which describes all the visual information available to an observer from every point in space and time.</p>

<p><img src="/img/lightfields/plenoptic_function_eye.png" alt="Visualization of 360 $^\circ$ observer taken from [1]" />
<em>Figure 1. Visualization of the Plenoptic Function for a 360 $^\circ$ observer taken from [1]</em></p>

<p>Now that we have a high level overview of the plenoptic function, we can define some terms. 
The first one is the term <strong>visual information</strong>. 
In the case of the plenoptic function visual information relates to the intensity distribution of the rays of light that pass through the observer (eye). 
The most simple parameterization of the plenoptic function is therefore, $\mathbf{P}\bigl(\theta, \phi\bigr)$, where $\mathbf{P}$ is the intensity distribution of the light rays, $\theta$ and $\phi$ are the spherical coordinates (polar angle and azimuthal angle) which parameterize the rays that pass through the eye.</p>

<p>Let’s extend the parameterization in two ways, we add the color of the rays and consider a sequence of pictures instead of a single image.
Thus, we add wavelength $\lambda$ and the time dimension $t$ to the current parameterization, we now have $\mathbf{P}\bigl(\theta,\phi,\lambda,t\bigr)$.</p>

<p>There is one last thing we should not forget and that is to add the parameters of the viewpoint (eye). 
The viewpoint, $V$, consists of three dimensions, $V_x, V_y, V_z$ and the plenoptic function becomes $\mathbf{P}\bigl(\theta,\phi,\lambda,t,V_x,V_y,V_z\bigr)$. 
The plenoptic function now describes the intensity of all rays observed by an observer for every point in time and space.</p>

<h2 id="light-field">Light Field</h2>
<p>The plenoptic function is useful in the sense that it is a more formal description of the human visual system from a computer science perspective.
However, at this point it is a rather complex 7D function.
Fortunately, we can simplify the function by adding some assumptions. 
First, we assume that there is no dispersion of light and that the color of a ray remains constant. 
Next, we eliminate the time parameter $t$, by only considering a specific moment in time (a snapshot). 
The resulting parameterization is $\mathbf{P}\bigl(\theta,\phi,V_x,V_y,V_z\bigr)$, which might look familiar if you have experience with light rays and ray tracing.</p>

<p>A simplification from 7D to 5D is interesting, however, we have not arrived yet at the definition of the light field function.
In 1996, a group of scientists came up with an interesting idea to place the scene in a transparent cube [2].
Now, we can limit our focus to the surface where a light ray enters and leaves the cube. 
In other words, a ray passes through a plane $(s,t)$, through the object, and exits through the plane $(u,v)$. 
We are only interested in the 2D positions of the point on the plane $(s,t)$ and $(u,v)$ and thus the parameterization consists of two 2D points.
The result is a 4D function also referred to as the Light Field Function or Lumigraph.</p>

<blockquote>
  <p>It is actually a bit more nuanced because in the light field function the scene is placed in a concave object and in the Lumigraph it is placed in a cube. However, these differences are mostly interesting when thinking about alternative parameterizations of the light field, e.g. a sphere or curved surface instead of a cube.</p>
</blockquote>

<p><img src="/img/lightfields/lumigraph_2dparameterization.png" alt="The Light Field Function taken from [2]" />
<em>Figure 2. Simplified overview of the Lumigraph taken from [2]</em></p>

<p>In short, the Light Field is a function that maps the color to each ray that intersects with the scene.</p>

<h2 id="receptive-fields">Receptive Fields</h2>
<p>The plenoptic function itself does not necessarily have many practical applications.
However, as mentioned earlier it is an approximation of the human visual system, and we know that the human visual system contains so called receptive fields.
These <a href="https://en.wikipedia.org/wiki/Receptive_field">receptive fields</a> “fire” when they encounter certain shapes (circles, squares etc.) or edges.
Let’s simplify the plenoptic function $\mathbf{P}\bigl(x,y, \theta,t,V_x,V_y,V_z\bigr)$ one more time, instead of spherical coordinates $\theta, \phi$ we use 
cartesian coordinates $x,y$. 
The cartesian coordinate notation is often used in computer vision and it represents the spatial coordinates of an image, or in other words the pixels.
We can extract interesting information from the plenoptic function by calculating the derivate with respect to certain parameters.
In order to extract the information, we first compute the local average and then the derivative (see [1] for an in depth explanation).
In other words, we apply a smoothing function (gaussian) on an image (Figure 3) and then convolve it with a kernel (Sobel).</p>

<p><img src="/img/lightfields/original.png" alt="The original image" />
<em>Figure 3. A simple image of a rectangle on the plane $\bigl(x,y\bigr)$</em></p>

<blockquote>
  <p>If you have a signals processing background or experience with convolutional neural networks, then this approach might sound familiar. The explanation that follows is in essence the application of Sobel filters on images.</p>
</blockquote>

<p>If we calculate the derivative with respect to $x$ or $y$, we can extract the horizontal and vertical information in the image.
We could also do the same for the diagonal direction, by deriving with respect to $x+y$. 
Furthermore, we can derive to different combinations of parameters to extract different visual information.
<!--dit moet je verbeteren-->
However, the derivatives with respect to $x$ and $y$ are the most practical to discuss since the other options are not trivial to implement and are more or less idealized receptive fields (see [2] page 10 ).</p>

<table align="center">
  <tr>
    <td>$D_x$</td>
    <td>$D_y$</td>
    <td>$D_{yx}$</td>
    <td>Laplacian</td>
  </tr>
  <tr>
    <td><img src="/img/lightfields/dx.png" width="100" /></td>
    <td><img src="/img/lightfields/dy.png" width="100" /></td>
    <td><img src="/img/lightfields/dyx.png" width="100" /></td>
    <td><img src="/img/lightfields/laplacian.png" width="100" /></td>
  </tr>
    <tr>
    <td>$D_{xx}$</td>
    <td>$D_{yy}$</td>
    <td>$D_{yyx}$</td>
    <td>$D_{yyxx}$</td>
  </tr>
  <tr>
    <td><img src="/img/lightfields/dxx.png" width="100" /></td>
    <td><img src="/img/lightfields/dyy.png" width="100" /></td>
    <td><img src="/img/lightfields/dyyx.png" width="100" /></td>
    <td><img src="/img/lightfields/dyyxx.png" width="100" /></td>
  </tr>
 </table>

<p><em>Table 1. An overview of the receptive fields which are the results of the derivatives of the plenoptic function. The $D$ notation represents the derivative and $D_{xx}$ is the second derivative w.r.t. x, notation is copied from [2]</em></p>

<p>In the table 1, we can see the receptive fields and when they are activated. 
We can see that $D_x$ and $D_y$ activate for horizontal and vertical edges, respectively.
The diagonal information can be extracted by deriving with respect to $y$ and $x$. 
The Laplacian is actually the sum of $D_{xx}$ and $D_{yy}$, it is the sum of the second order derivates in the horizontal and vertical direction.
The receptive fields are interesting because they can also be described as feature detectors or kernels.
These kernels are convolved over the image and the rate of activation depends on whether the features that the kernel is sensitive for exist in the image. 
The key point to remember is that the receptive fields act as feature detectors (kernels), and stem from the early stages of visual processing in the human visual system.</p>

<blockquote>
  <p>The convolutional neural networks (CNN) could be described as stacked layers of feature detectors and the idea for this approach comes from the human visual system.</p>
</blockquote>

<p>This has mostly been a theoretical review of the plenoptic function, however, the Lumigraph is a rendering function based on the plenoptic function. If you are interested in applying the theory, I would like to refer you to section 3 of [2] which provides information to implement the Lumigraph System (algorithmic level).</p>

<h2 id="tying-it-all-together">Tying it all together</h2>
<p>We have covered the theory behind the Plenoptic Function, Light Field and Lumigraph. 
We have also explained how we can extract visual information from the Plenoptic Function in a manner that resembles how the human visual system works. 
The plenoptic function is interesting because it is a continuous 3D representation of a scene. 
It is a computational model of how vision works, translating this model to the algorithmic and implementational level would give you the ability to construct Light Fields for a scene and render all views of that scene. 
In the upcoming articles we will continue with continuous 3D representations and explore how neural networks can be used to construct the Light Field of a certain scene.</p>

<p>To summarize, we started at the computational level with the Plenoptic function which describes the human visual system from a computer scientists perspective. 
We continued with a simplification of the plenoptic function, resulting in the Light Field and Lumigraph. 
The Light Field is still at the computational level whereas the Lumigraph contains an actual rendering framework. 
However, we will explore other 3D rendering frameworks in future work and the concepts in this article have been introduced as background knowledge for these 3D representations.</p>

<h2 id="references">References</h2>

<p>[1] Edward H Adelson and James R Bergen. The plenoptic function and the elements of early vision,
1991.</p>

<p>[2] Michael Cohen, Steven J. Gortler, Richard Szeliski, Radek Grzeszczuk, and Rick Szeliski. The
lumigraph. Association for Computing Machinery, Inc., August 1996.</p>

          </section>

          
<!-- Share Buttons Start -->
<div>
  <ul class="rrssb-buttons clearfix">
    
      <li class="rrssb-email">
        <a href="mailto:?subject=Computer Science approximation of the human visual system&body=https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html" data-proofer-ignore>
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 28 28">
              <path d="M20.11 26.147c-2.335 1.05-4.36 1.4-7.124 1.4C6.524 27.548.84 22.916.84 15.284.84 7.343 6.602.45 15.4.45c6.854 0 11.8 4.7 11.8 11.252 0 5.684-3.193 9.265-7.398 9.3-1.83 0-3.153-.934-3.347-2.997h-.077c-1.208 1.986-2.96 2.997-5.023 2.997-2.532 0-4.36-1.868-4.36-5.062 0-4.75 3.503-9.07 9.11-9.07 1.713 0 3.7.4 4.6.972l-1.17 7.203c-.387 2.298-.115 3.3 1 3.4 1.674 0 3.774-2.102 3.774-6.58 0-5.06-3.27-8.994-9.304-8.994C9.05 2.87 3.83 7.545 3.83 14.97c0 6.5 4.2 10.2 10 10.202 1.987 0 4.09-.43 5.647-1.245l.634 2.22zM16.647 10.1c-.31-.078-.7-.155-1.207-.155-2.572 0-4.596 2.53-4.596 5.53 0 1.5.7 2.4 1.9 2.4 1.44 0 2.96-1.83 3.31-4.088l.592-3.72z"
              />
            </svg>
          </span>
          <span class="rrssb-text">email</span>
        </a>
      </li>
    
    
    
      <li class="rrssb-twitter">
        <a href="https://twitter.com/share?url=https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html&text=Computer Science approximation of the human visual system"
        class="popup">
          <span class="rrssb-icon"><svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 28 28"><path d="M24.253 8.756C24.69 17.08 18.297 24.182 9.97 24.62c-3.122.162-6.22-.646-8.86-2.32 2.702.18 5.375-.648 7.507-2.32-2.072-.248-3.818-1.662-4.49-3.64.802.13 1.62.077 2.4-.154-2.482-.466-4.312-2.586-4.412-5.11.688.276 1.426.408 2.168.387-2.135-1.65-2.73-4.62-1.394-6.965C5.574 7.816 9.54 9.84 13.802 10.07c-.842-2.738.694-5.64 3.434-6.48 2.018-.624 4.212.043 5.546 1.682 1.186-.213 2.318-.662 3.33-1.317-.386 1.256-1.248 2.312-2.4 2.942 1.048-.106 2.07-.394 3.02-.85-.458 1.182-1.343 2.15-2.48 2.71z"/></svg></span>
          <span class="rrssb-text">twitter</span>
        </a>
      </li>
    
    
      <li class="rrssb-linkedin">
      <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html" class="popup">
        <span class="rrssb-icon">
          <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 28 28">
            <path d="M25.424 15.887v8.447h-4.896v-7.882c0-1.98-.71-3.33-2.48-3.33-1.354 0-2.158.91-2.514 1.802-.13.315-.162.753-.162 1.194v8.216h-4.9s.067-13.35 0-14.73h4.9v2.087c-.01.017-.023.033-.033.05h.032v-.05c.65-1.002 1.812-2.435 4.414-2.435 3.222 0 5.638 2.106 5.638 6.632zM5.348 2.5c-1.676 0-2.772 1.093-2.772 2.54 0 1.42 1.066 2.538 2.717 2.546h.032c1.71 0 2.77-1.132 2.77-2.546C8.056 3.593 7.02 2.5 5.344 2.5h.005zm-2.48 21.834h4.896V9.604H2.867v14.73z"
            />
          </svg>
        </span>
        <span class="rrssb-text">linkedin</span>
      </a>
      </li>
    
    
      <li class="rrssb-reddit">
        <a href="https://www.reddit.com/submit?url=https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html&title=Computer Science approximation of the human visual system" target="_blank">
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 28 28">
              <path d="M11.794 15.316c0-1.03-.835-1.895-1.866-1.895-1.03 0-1.893.866-1.893 1.896s.863 1.9 1.9 1.9c1.023-.016 1.865-.916 1.865-1.9zM18.1 13.422c-1.03 0-1.895.864-1.895 1.895 0 1 .9 1.9 1.9 1.865 1.03 0 1.87-.836 1.87-1.865-.006-1.017-.875-1.917-1.875-1.895zM17.527 19.79c-.678.68-1.826 1.007-3.514 1.007h-.03c-1.686 0-2.834-.328-3.51-1.005-.264-.265-.693-.265-.958 0-.264.265-.264.7 0 1 .943.9 2.4 1.4 4.5 1.402.005 0 0 0 0 0 .005 0 0 0 0 0 2.066 0 3.527-.46 4.47-1.402.265-.264.265-.693.002-.958-.267-.334-.688-.334-.988-.043z"
              />
              <path d="M27.707 13.267c0-1.785-1.453-3.237-3.236-3.237-.792 0-1.517.287-2.08.76-2.04-1.294-4.647-2.068-7.44-2.218l1.484-4.69 4.062.955c.07 1.4 1.3 2.6 2.7 2.555 1.488 0 2.695-1.208 2.695-2.695C25.88 3.2 24.7 2 23.2 2c-1.06 0-1.98.616-2.42 1.508l-4.633-1.09c-.344-.082-.693.117-.803.454l-1.793 5.7C10.55 8.6 7.7 9.4 5.6 10.75c-.594-.45-1.3-.75-2.1-.72-1.785 0-3.237 1.45-3.237 3.2 0 1.1.6 2.1 1.4 2.69-.04.27-.06.55-.06.83 0 2.3 1.3 4.4 3.7 5.9 2.298 1.5 5.3 2.3 8.6 2.325 3.227 0 6.27-.825 8.57-2.325 2.387-1.56 3.7-3.66 3.7-5.917 0-.26-.016-.514-.05-.768.965-.465 1.577-1.565 1.577-2.698zm-4.52-9.912c.74 0 1.3.6 1.3 1.3 0 .738-.6 1.34-1.34 1.34s-1.343-.602-1.343-1.34c.04-.655.596-1.255 1.396-1.3zM1.646 13.3c0-1.038.845-1.882 1.883-1.882.31 0 .6.1.9.21-1.05.867-1.813 1.86-2.26 2.9-.338-.328-.57-.728-.57-1.26zm20.126 8.27c-2.082 1.357-4.863 2.105-7.83 2.105-2.968 0-5.748-.748-7.83-2.105-1.99-1.3-3.087-3-3.087-4.782 0-1.784 1.097-3.484 3.088-4.784 2.08-1.358 4.86-2.106 7.828-2.106 2.967 0 5.7.7 7.8 2.106 1.99 1.3 3.1 3 3.1 4.784C24.86 18.6 23.8 20.3 21.8 21.57zm4.014-6.97c-.432-1.084-1.19-2.095-2.244-2.977.273-.156.59-.245.928-.245 1.036 0 1.9.8 1.9 1.9-.016.522-.27 1.022-.57 1.327z"
              />
            </svg>
          </span>
          <span class="rrssb-text">reddit</span>
        </a>
      </li>
    
    
      <li class="rrssb-googleplus">
        <a href="https://plus.google.com/share?url=https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html" class="popup">
          <span class="rrssb-icon">
            <svg xmlns="https://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M21 8.29h-1.95v2.6h-2.6v1.82h2.6v2.6H21v-2.6h2.6v-1.885H21V8.29zM7.614 10.306v2.925h3.9c-.26 1.69-1.755 2.925-3.9 2.925-2.34 0-4.29-2.016-4.29-4.354s1.885-4.353 4.29-4.353c1.104 0 2.014.326 2.794 1.105l2.08-2.08c-1.3-1.17-2.924-1.883-4.874-1.883C3.65 4.586.4 7.835.4 11.8s3.25 7.212 7.214 7.212c4.224 0 6.953-2.988 6.953-7.082 0-.52-.065-1.104-.13-1.624H7.614z"/></svg>
          </span>
          <span class="rrssb-text">google+</span>
        </a>
      </li>
    
    
    
    
      <li class="rrssb-pocket">
        <a href="https://getpocket.com/save?url=https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html">
          <span class="rrssb-icon">
            <svg width="32" height="28" viewBox="0 0 32 28" xmlns="http://www.w3.org/2000/svg">
              <path d="M28.782.002c2.03.002 3.193 1.12 3.182 3.106-.022 3.57.17 7.16-.158 10.7-1.09 11.773-14.588 18.092-24.6 11.573C2.72 22.458.197 18.313.057 12.937c-.09-3.36-.05-6.72-.026-10.08C.04 1.113 1.212.016 3.02.008 7.347-.006 11.678.004 16.006.002c4.258 0 8.518-.004 12.776 0zM8.65 7.856c-1.262.135-1.99.57-2.357 1.476-.392.965-.115 1.81.606 2.496 2.453 2.334 4.91 4.664 7.398 6.966 1.086 1.003 2.237.99 3.314-.013 2.407-2.23 4.795-4.482 7.17-6.747 1.203-1.148 1.32-2.468.365-3.426-1.01-1.014-2.302-.933-3.558.245-1.596 1.497-3.222 2.965-4.75 4.526-.706.715-1.12.627-1.783-.034-1.597-1.596-3.25-3.138-4.93-4.644-.47-.42-1.123-.647-1.478-.844z"
              />
            </svg>
          </span>
          <span class="rrssb-text">pocket</span>
        </a>
      </li>
    
    
  </ul>
</div>

<!-- Share Buttons End -->


          
            
<!-- Disqus Comments Start -->


  <div id="disqus_thread"></div>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


<!-- Disqus Comments End -->

          


          <hr />

          
        </div>
      </div>
    </section>

    <!-- Footer Start -->

<footer>

    <!-- Social Buttons Start -->

<ul class="list-inline social-buttons">
    
    <li><a href="https://github.com/Avuerro/" target="_blank"><i class="fa fa-github fa-fw"></i></a></li>
    
    <li><a href="https://www.linkedin.com/in/chihab-amghane/" target="_blank"><i class="fa fa-linkedin fa-fw"></i></a></li>
    
    
</ul>

<!-- Social Buttons End -->


    <p><br><br></p>

    <div class="container text-center">
        <p>Copyright &copy; Chihab Amghane 2022</p>
        <!--  -->
    </div>
</footer>

<p></p>

<!-- Footer End -->


    
<!-- Javascript Start -->

<!-- jQuery -->
<script src="https://code.jquery.com/jquery-1.11.3.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

<!-- Custom Theme JavaScript -->
<!--* Start Bootstrap - Grayscale Bootstrap Theme (http://startbootstrap.com)
* Code licensed under the Apache License v2.0.
* For details, see http://www.apache.org/licenses/LICENSE-2.0.-->
<script>
function toggleNavCollapse(){50<$(".navbar").offset().top?$(".navbar-fixed-top").addClass("top-nav-collapse"):$(".navbar-fixed-top").removeClass("top-nav-collapse");}
$(document).ready(toggleNavCollapse);
$(window).scroll(toggleNavCollapse);$(function(){$("a.page-scroll").bind("click",function(b){var a=$(this);$("html, body").stop().animate({scrollTop:$(a.attr("href")).offset().top-50},1500,"easeInOutExpo",function(){a.blur()});b.preventDefault()})});$(".navbar-collapse ul li a").click(function(){$(".navbar-toggle:visible").click()});
</script>





  <!-- Syntax highlight in post pages-->

  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.2/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>




  

  <!-- Google Tracking Id Start -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>

  <!-- Google Tracking Id End -->

  




  <!-- Disqus -->

  

    <script type="text/javascript">
    var disqus_shortname = 'personal-jekyll-theme';
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>

  

  

    <!-- Comments Counter Start -->

    <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'personal-jekyll-theme'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>

    <!-- Comments Counter End -->

  





  <!-- Share buttons Start -->

  <script src="/js/rrssb.min.js"></script>

  <!-- Share buttons End -->





<script>
function addTohistory() {
  if (!window.location.host.startsWith("127.0.0.1")) {
    history.pushState({}, 'Computer Science approximation of the human visual system', 'https://avuerro.github.io/computer%20vision/2022/09/07/Light-Field.html');
  }
}
</script>

<!-- Gesture Navigation / Swipe Instruction Start -->


  

    <!-- Post Gesture Navigation Start -->

    <script type="text/javascript" src="/js/hammer.min.js"></script>

    <script>
      var post = document.getElementById('post');

      new Hammer(post).on('swipeleft', function(event) {
        addTohistory();
        
          document.location.replace("/");
        
      });

      new Hammer(post).on('swiperight', function(event) {
        addTohistory();
        
          document.location.replace("/");
        
      });
    </script>

    <!-- Post Gesture Navigation Start -->

  

  

  

  

    <!-- Swipe Instructions for Post Start -->

    <script>
      $(document).ready(function(){
        if(!localStorage.getItem('post-swipeshowed')){
          $("#swipe-instruction").fadeIn();
          $("#swipe-instruction .close-swipe-instruction").click(function(){
            $("#swipe-instruction").fadeOut();
          });
          localStorage.setItem('post-swipeshowed', true);
        }
      });
    </script>

    <!-- Swipe Instructions for Post End -->

  

<!-- Gesture Navigation / Swipe Instruction End -->



<!-- Javascript End -->


  </body>
</html>

<!-- Post Layout End -->
